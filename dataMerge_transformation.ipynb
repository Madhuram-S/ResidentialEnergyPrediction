{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Script to transform data\n",
    "\n",
    "The script focuses on bring RECS data from 2001, 2005, 2009 and 2015 into \n",
    "1. common format\n",
    "2. columns mapped correctly to the correct fields\n",
    "3. calaculations done as required\n",
    "4. elimination of columns not needed\n",
    "\n",
    "### Rules:\n",
    "1.\tAll columns starting with \"Z\" is dropped\n",
    "2.\tYearMade is converted to a range as per 2015 standards\n",
    "3.\tDrop 1997\n",
    "4.\tClub all electronics into 3 categories TVREL, PCOFFEQUIP, PHONE\n",
    "5.\tAll comsumption will be BTU and KWH will not be used\n",
    "6.\tCalucaled Fields\n",
    "    - \"2001 - TOTROOMS - sum(BEDROOMS,NCOMBATH,NHAFBATH,OTHROOMS)\"\n",
    "\t- TOTALBTU = TOTALBTUSPH + TOTALBTUWTH + TOTALBTUOTH\n",
    "\t- TOTALBTUSPH = BTULPSPH + BTUNGSPH + BTUFOSPH + BTUELSPH\n",
    "\t- TOTALBTUWTH = BTULPWTH + BTUNGWTH + BTUFOWTH + BTUELWTH\n",
    "\t- TOTALBTUOTH = BTULPAPL + BTUNGOTH + BTUFOAPL + BTUELOTH + BTUELFRG +BTUELCOL\n",
    "\t- TOTALDOLLAR = TOTALBTUSPH + TOTALBTUWTH + TOTALBTUOTH\n",
    "\t- TOTALDOLSPH = DOLLPSPH+ DOLNGSPH + DOLFOSPH + DOLELSPH\n",
    "\t- TOTALDOLWTH = DOLLPWTH+ DOLNGWTH+ DOLFOWTH+ DOLELWTH\n",
    "\t- TOTALDOLOTH = DOLLPOTH + DOLNGOTH+ DOLFOOTH + DOLELOTH + DOLELFRG + DOLELCOL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all dependecies\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Notebook Variables and Initialization\n",
    "dataFilePath = \"dataforfinalproject/RawDataFiles\"\n",
    "\n",
    "codebook_path = \"dataforfinalproject/RawDataFiles/Codebooks\"\n",
    "\n",
    "years = [2001, 2005, 2009, 2015]\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "dataFiles = ['target_final_2001.csv', \"RECS05alldata.csv\", \"recs2009_public.csv\", \"recs2015_public_v4.csv\"]\n",
    "\n",
    "col_list = ['2001_requiredCols.txt', '2005_requiredCols.txt', '2009_requiredCols.txt', '2015_requiredCols.txt']\n",
    "\n",
    "final_colList = \"final_columnList.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n",
      "2005\n",
      "2009\n",
      "2015\n"
     ]
    }
   ],
   "source": [
    "# read the files into a dataframe and compress dataframe to have only required cols\n",
    "for i, y in enumerate(years):\n",
    "    print(y)\n",
    "    df = pd.read_csv(os.path.join(dataFilePath, dataFiles[i]), low_memory=False)\n",
    "    l = pd.read_csv(os.path.join(codebook_path, col_list[i]), header= None, names = ['cols']).cols.tolist()\n",
    "    dfs[y] = df[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[2005] = pd.read_csv(os.path.join(dataFilePath, dataFiles[1]), low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format and tranform 2001 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Year of Survey to each of the datasets\n",
    "for y in years:\n",
    "    dfs[y]['RECSYEAR'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change columnname YearMade to YearMadeRange\n",
    "dfs[2015].rename(columns = {\"YEARMADERANGE\":\"YEARMADE\"}, inplace = True)\n",
    "\n",
    "# change columnname Basefin to Cellar\n",
    "dfs[2005].rename(columns = {\"BASEFIN\":\"CELLAR\"}, inplace = True)\n",
    "dfs[2015].rename(columns = {\"BASEFIN\":\"CELLAR\"}, inplace = True)\n",
    "#dfs[2015].YEARMADE.head()\n",
    "\n",
    "# change OCCUPYYRANGE to OCCUPYY\n",
    "dfs[2015].rename(columns = {\"OCCUPYYRANGE\":\"OCCUPYY\"}, inplace = True)\n",
    "dfs[2009].rename(columns = {\"OCCUPYYRANGE\":\"OCCUPYY\"}, inplace = True)\n",
    "\n",
    "# change USENG to UGASHERE\n",
    "dfs[2009].rename(columns = {\"USENG\":\"UGASHERE\"}, inplace = True)\n",
    "\n",
    "# change TOAST to TOASTER in 2015\n",
    "dfs[2015].rename(columns = {\"TOAST\":\"TOASTER\"}, inplace = True)\n",
    "\n",
    "# change \"STOVENFU\" to \"STOVENFUEL\"\n",
    "dfs[2001].rename(columns = {\"STOVENFU\":\"STOVENFUEL\"}, inplace = True)\n",
    "dfs[2005].rename(columns = {\"STOVENFU\":\"STOVENFUEL\"}, inplace = True)\n",
    "\n",
    "# rename columns OUTLGTNT(2001), NOUTLGTNT(2005, 2009) to LGTOUTNUM\n",
    "dfs[2001].rename(columns = {\"OUTLGTNT\":\"LGTOUTNUM\"}, inplace = True)\n",
    "dfs[2005].rename(columns = {\"NOUTLGTNT\":\"LGTOUTNUM\"}, inplace = True)\n",
    "dfs[2009].rename(columns = {\"NOUTLGTNT\":\"LGTOUTNUM\"}, inplace = True)\n",
    "\n",
    "# rename ORIGIN1 to Householder_Race in 2001 and 2005\n",
    "dfs[2001].rename(columns = {\"ORIGIN1\":\"HOUSEHOLDER_RACE\"}, inplace = True)\n",
    "dfs[2005].rename(columns = {\"ORIGIN1\":\"HOUSEHOLDER_RACE\"}, inplace = True)\n",
    "dfs[2009].rename(columns = {\"Householder_Race\":\"HOUSEHOLDER_RACE\"}, inplace = True)\n",
    "\n",
    "# rename CD65 and HD65 to CDD65 and HDD65\n",
    "dfs[2005].rename(columns = {\"CD65\":\"CDD65\", \"HD65\":\"HDD65\"}, inplace = True)\n",
    "\n",
    "# rename CORDS TO WOODAMT in 2001\n",
    "dfs[2001].rename(columns = {\"CORDS\":\"WOODAMT\"}, inplace = True)\n",
    "\n",
    "#change NGPAY to PGASHEAT for 2015\n",
    "dfs[2015].rename(columns = {\"NGPAY\": \"PGASHEAT\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate totrooms for 2001 data\n",
    "dfs[2001]['TOTROOMS'] = dfs[2001].BEDROOMS + dfs[2001].NCOMBATH  + dfs[2001].NHAFBATH +dfs[2001].OTHROOMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "equipAux_type = [\"REVERSE\",\"WARMAIR\",\"STEAMR\",\"PERMELEC\",\"PIPELESS\",\"ROOMHEAT\",\"WOODKILN\",\"CARRYEL\",\"CARRYKER\",\n",
    "                 \"CHIMNEY\",\"RANGE\",\"DIFEQUIP\"]\n",
    "for e in equipAux_type:\n",
    "    dfs[2015][e] = 0\n",
    "    \n",
    "dfs[2015].loc[dfs[2015].EQUIPAUXTYPE == 1,\"CARRYEL\"] = 1\n",
    "dfs[2015].loc[dfs[2015].EQUIPAUXTYPE == 2,'WOODKILN'] = 1\n",
    "dfs[2015].loc[dfs[2015].EQUIPAUXTYPE == 3,'PIPELESS'] = 1\n",
    "dfs[2015].loc[dfs[2015].EQUIPAUXTYPE == 4,'CHIMNEY'] = 1\n",
    "dfs[2015].loc[dfs[2015].EQUIPAUXTYPE == 9,'DIFEQUIP'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For EquipAux types in 2001 to 2009, if value is -2 change it to 0\n",
    "for e in equipAux_type:\n",
    "    dfs[2001].loc[dfs[2001][e] == -2,e] = 0\n",
    "    dfs[2005].loc[dfs[2005][e] == -2,e] = 0\n",
    "    dfs[2009].loc[dfs[2009][e] == -2,e] = 0   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "equipAux_fuel = [\"ELECAUX\",\"UGASAUX\",\"LPGAUX\",\"FOKRAUX\",\"WOODAUX\",\"OTHERAUX\"]\n",
    "\n",
    "for e in equipAux_fuel:\n",
    "    dfs[2015][e] = 0\n",
    "    dfs[2009][e] = 0\n",
    "    \n",
    "\n",
    "#2015 changes\n",
    "dfs[2015].loc[dfs[2015].FUELAUX == 1,\"UGASAUX\"] = 1\n",
    "dfs[2015].loc[dfs[2015].FUELAUX == 2,'LPGAUX'] = 1\n",
    "dfs[2015].loc[dfs[2015].FUELAUX == 3,'FOKRAUX'] = 1\n",
    "dfs[2015].loc[dfs[2015].FUELAUX == 5,'ELECAUX'] = 1\n",
    "dfs[2015].loc[dfs[2015].FUELAUX == 7,'WOODAUX'] = 1\n",
    "dfs[2015].loc[dfs[2015].FUELAUX == 21,'OTHERAUX'] = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes to 2001 to 2005\n",
    "# combine the value for FO and Kero to one column and Solar into Other columns\n",
    "\n",
    "dfs[2001]['FOKRAUX'] = 0\n",
    "dfs[2005]['FOKRAUX'] = 0\n",
    "\n",
    "\n",
    "dfs[2001].loc[(dfs[2001]['FOILAUX'] == 1) | (dfs[2001]['KEROAUX'] == 1),\"FOKRAUX\"] = 1\n",
    "dfs[2005].loc[(dfs[2005]['FOILAUX'] == 1) | (dfs[2005]['KEROAUX'] == 1),\"FOKRAUX\"] = 1\n",
    "\n",
    "dfs[2001].loc[(dfs[2001]['SOLARAUX'] == 1) | (dfs[2001]['OTHERAUX'] == 1),\"OTHERAUX\"] = 1\n",
    "dfs[2005].loc[(dfs[2005]['SOLARAUX'] == 1) | (dfs[2005]['OTHERAUX'] == 1),\"OTHERAUX\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in equipAux_fuel:\n",
    "    dfs[2001].loc[dfs[2001][e] == -2,e] = 0\n",
    "    dfs[2005].loc[dfs[2005][e] == -2,e] = 0   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform 2009 Auxillary fuel type to standard fuel types as specified in years (2001, 2005 and 2015)\n",
    "equipAux_fuel_2009 = [\"FURNFUEL\",\"RADFUEL\",\"PIPEFUEL\",\"RMHTFUEL\",\"HSFUEL\",\"FPFUEL\",\"RNGFUEL\",\"DIFFUEL\"]\n",
    "for e in equipAux_fuel_2009:\n",
    "    dfs[2009].loc[dfs[2009][e] == 1,\"UGASAUX\"] = 1\n",
    "    dfs[2009].loc[dfs[2009][e] == 2,'LPGAUX'] = 1\n",
    "    dfs[2009].loc[(dfs[2009][e] == 3) | (dfs[2009][e] == 4) ,'FOKRAUX'] = 1\n",
    "    dfs[2009].loc[dfs[2009][e] == 5,'ELECAUX'] = 1\n",
    "    dfs[2009].loc[dfs[2009][e] == 7,'WOODAUX'] = 1\n",
    "    dfs[2009].loc[dfs[2009][e] > 7,'OTHERAUX'] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7\n",
       "1    6\n",
       "2    3\n",
       "3    5\n",
       "4    5\n",
       "Name: YEARMADE, dtype: category\n",
       "Categories (8, int64): [1 < 2 < 3 < 4 < 5 < 6 < 7 < 8]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The year Column in 2009 is actual year instead of coded so convert to Year Category\n",
    "yr_bins = [0, 1951, 1960,1970, 1980,1990, 2000, 2010, 2019]\n",
    "yr_labels = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "dfs[2009].rename(columns = {\"YEARMADE\": \"YRBUILT\"}, inplace = True)\n",
    "dfs[2009]['YEARMADE'] = pd.cut(dfs[2009]['YRBUILT'], labels = yr_labels, bins = yr_bins)\n",
    "dfs[2009].YEARMADE.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the value for Energy Star Dishwasher for 2001 based on other years\n",
    "# currently go with -9 as no data has been recorded for this\n",
    "dfs[2001]['ESDISHW'] = -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all categories of BTU usage into BTUXXOTH\n",
    "# --- Electric\n",
    "dfs[2001]['BTUELOTH'] = dfs[2001][[\"BTUELAPL\",\"BTUELFZZ\",\"BTUELDWH\",\"BTUELCDR\"]].sum(axis = 1)\n",
    "dfs[2005]['BTUELOTH'] = dfs[2005][[\"BTUELAPL\",\"BTUELFZZ\",\"BTUELDWH\",\"BTUELCDR\"]].sum(axis = 1)\n",
    "dfs[2015]['BTUELOTH'] = dfs[2015][[\"BTUELRFG1\",\"BTUELRFG2\",\"BTUELFRZ\",\"BTUELCOK\",\"BTUELMICRO\",\"BTUELCW\",\"BTUELCDR\",\\\n",
    "                                \"BTUELDWH\",\"BTUELLGT\",\"BTUELTVREL\",\"BTUELTV1\",\"BTUELTV2\",\"BTUELAHUHEAT\",\"BTUELAHUCOL\",\\\n",
    "                                   \"BTUELEVAPCOL\",\"BTUELCFAN\",\"BTUELDHUM\",\"BTUELHUM\",\"BTUELPLPMP\",\"BTUELHTBPMP\",\"BTUELHTBHEAT\",\\\n",
    "                                   \"BTUELNEC\"]].sum(axis = 1)\n",
    "\n",
    "# --- LPG\n",
    "dfs[2001].rename(columns = {\"BTULPAPL\":\"BTULPOTH\"}, inplace = True)\n",
    "dfs[2005].rename(columns = {\"BTULPAPL\":\"BTULPOTH\"}, inplace = True)\n",
    "dfs[2015]['BTULPOTH'] = dfs[2015][[\"BTULPCOK\",\"BTULPCDR\",\"BTULPNEC\"]].sum(axis = 1)\n",
    "\n",
    "# --- Natural Gas\n",
    "dfs[2001].rename(columns = {\"BTUNGAPL\":\"BTUNGOTH\"}, inplace = True)\n",
    "dfs[2005].rename(columns = {\"BTUNGAPL\":\"BTUNGOTH\"}, inplace = True)\n",
    "dfs[2015]['BTUNGOTH'] = dfs[2015][[\"BTUNGCOK\",\"BTUNGCDR\",\"BTUNGPLHEAT\",\"BTUNGHTBHEAT\",\"BTUNGNEC\"]].sum(axis = 1)\n",
    "\n",
    "# --- Fuel Oil\n",
    "dfs[2001]['BTUFOOTH'] = dfs[2001][[\"BTUFOAPL\",\"BTUKRAPL\"]].sum(axis = 1)\n",
    "dfs[2005].rename(columns = {\"BTUFOAPL\":\"BTUFOOTH\"}, inplace = True)\n",
    "dfs[2015].rename(columns = {\"BTUFONEC\":\"BTUFOOTH\"}, inplace = True)\n",
    "\n",
    "# for 2001, merge the total for BTU from FO and Kerosene into one col\n",
    "dfs[2001]['BTUFOSPH'] = dfs[2001][[\"BTUFOSPH\",\"BTUKRSPH\"]].sum(axis = 1)\n",
    "dfs[2001]['BTUFOWTH'] = dfs[2001][[\"BTUFOWTH\",\"BTUKRWTH\"]].sum(axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all categories of DOL usage into DOLXXOTH\n",
    "# --- Electric\n",
    "dfs[2001]['DOLELOTH'] = dfs[2001][[\"DOLELAPL\",\"DOLELFZZ\",\"DOLELDWH\",\"DOLELCDR\"]].sum(axis = 1)\n",
    "dfs[2005]['DOLELOTH'] = dfs[2005][[\"DOLELAPL\",\"DOLELFZZ\",\"DOLELDWH\",\"DOLELCDR\"]].sum(axis = 1)\n",
    "dfs[2015]['DOLELOTH'] = dfs[2015][[\"DOLELRFG1\",\"DOLELRFG2\",\"DOLELFRZ\",\"DOLELCOK\",\"DOLELMICRO\",\"DOLELCW\",\"DOLELCDR\",\\\n",
    "                                   \"DOLELDWH\",\"DOLELLGT\",\"DOLELTVREL\",\"DOLELTV1\",\"DOLELTV2\",\"DOLELAHUHEAT\",\"DOLELAHUCOL\",\\\n",
    "                                   \"DOLELCFAN\",\"DOLELDHUM\",\"DOLELHUM\",\"DOLELPLPMP\",\"DOLELHTBPMP\",\"DOLELHTBHEAT\",\\\n",
    "                                   \"DOLELNEC\"]].sum(axis = 1)\n",
    "\n",
    "# --- LPG\n",
    "dfs[2001].rename(columns = {\"DOLLPAPL\":\"DOLLPOTH\"}, inplace = True)\n",
    "dfs[2005].rename(columns = {\"DOLLPAPL\":\"DOLLPOTH\"}, inplace = True)\n",
    "dfs[2015]['DOLLPOTH'] = dfs[2015][[\"DOLLPCOK\",\"DOLLPCDR\",\"DOLLPNEC\"]].sum(axis = 1)\n",
    "\n",
    "# --- Natural Gas\n",
    "dfs[2001].rename(columns = {\"DOLNGAPL\":\"DOLNGOTH\"}, inplace = True)\n",
    "dfs[2005].rename(columns = {\"DOLNGAPL\":\"DOLNGOTH\"}, inplace = True)\n",
    "dfs[2015]['DOLNGOTH'] = dfs[2015][[\"DOLNGCOK\",\"DOLNGCDR\",\"DOLNGPLHEAT\",\"DOLNGHTBHEAT\",\"DOLNGNEC\"]].sum(axis = 1)\n",
    "\n",
    "# --- Fuel Oil\n",
    "dfs[2001]['DOLFOOTH'] = dfs[2001][[\"DOLFOAPL\",\"DOLKRAPL\"]].sum(axis = 1)\n",
    "dfs[2005].rename(columns = {\"DOLFOAPL\":\"DOLFOOTH\"}, inplace = True)\n",
    "dfs[2015].rename(columns = {\"DOLFONEC\":\"DOLFOOTH\"}, inplace = True)\n",
    "\n",
    "# for 2001, merge the total for DOL from FO and Kerosene into one col\n",
    "dfs[2001]['DOLFOSPH'] = dfs[2001][[\"DOLFOSPH\",\"DOLKRSPH\"]].sum(axis = 1)\n",
    "dfs[2001]['DOLFOWTH'] = dfs[2001][[\"DOLFOWTH\",\"DOLKRWTH\"]].sum(axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the TOTAL BTU VALUES\n",
    "# --- BY end use\n",
    "# TOTALBTU = TOTALBTUSPH + TOTALBTUWTH + TOTALBTUOTH\n",
    "# TOTALBTUSPH = BTULPSPH + BTUNGSPH + BTUFOSPH + BTUELSPH\n",
    "# TOTALBTUWTH = BTULPWTH + BTUNGWTH + BTUFOWTH + BTUELWTH\n",
    "# TOTALBTUOTH = BTULPAPL + BTUNGOTH + BTUFOAPL + BTUELOTH + BTUELFRG +BTUELCOL\n",
    "# TOTALDOLLAR = TOTALDOLSPH + TOTALDOLWTH + TOTALDOLOTH\n",
    "# TOTALDOLSPH = DOLLPSPH+ DOLNGSPH + DOLFOSPH + DOLELSPH\n",
    "# TOTALDOLWTH = DOLLPWTH+ DOLNGWTH+ DOLFOWTH+ DOLELWTH\n",
    "# TOTALDOLOTH = DOLLPOTH + DOLNGOTH+ DOLFOOTH + DOLELOTH + DOLELFRG + DOLELCOL\n",
    "\n",
    "# --- Totals by end use\n",
    "for y in years:\n",
    "    dfs[y]['TOTALBTUSPH'] = dfs[y][[\"BTULPSPH\",\"BTUNGSPH\",\"BTUFOSPH\",\"BTUELSPH\"]].sum(axis = 1)\n",
    "    dfs[y]['TOTALBTUWTH'] = dfs[y][[\"BTULPWTH\",\"BTUNGWTH\",\"BTUFOWTH\",\"BTUELWTH\"]].sum(axis = 1)\n",
    "    dfs[y]['TOTALBTUOTH'] = dfs[y][[\"BTULPOTH\",\"BTUNGOTH\",\"BTUFOOTH\",\"BTUELOTH\",\"BTUELCOL\",\"BTUELRFG\"]].sum(axis = 1)\n",
    "    dfs[y]['TOTALBTU'] = dfs[y][[\"TOTALBTUSPH\",\"TOTALBTUWTH\",\"TOTALBTUOTH\"]].sum(axis = 1)\n",
    "    dfs[y]['TOTALDOLSPH'] = dfs[y][[\"DOLLPSPH\",\"DOLNGSPH\",\"DOLFOSPH\",\"DOLELSPH\"]].sum(axis = 1)\n",
    "    dfs[y]['TOTALDOLWTH'] = dfs[y][[\"DOLLPWTH\",\"DOLNGWTH\",\"DOLFOWTH\",\"DOLELWTH\"]].sum(axis = 1)\n",
    "    dfs[y]['TOTALDOLOTH'] = dfs[y][[\"DOLLPOTH\",\"DOLNGOTH\",\"DOLFOOTH\",\"DOLELOTH\",\"DOLELCOL\",\"DOLELRFG\"]].sum(axis = 1)\n",
    "    dfs[y]['TOTALDOLLAR'] = dfs[y][[\"TOTALDOLSPH\",\"TOTALDOLWTH\",\"TOTALDOLOTH\"]].sum(axis = 1)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine TV/AUDIO/STEREO into one col TVAUDIOEQUIP\n",
    "# 2001 - \"TVCOLOR\", \"BIGTV\", \"VCR\",\"STEREO\",\"BOOMBOX\",\"COMPCTST\",\"COMPNTST\",\"OTHSTER\"   \n",
    "\n",
    "dfs[2001]['TVAUDIOEQUIP'] = dfs[2001][[\"TVCOLOR\", \"BIGTV\", \"VCR\",\"STEREO\",\"BOOMBOX\",\"COMPCTST\",\"COMPNTST\",\"OTHSTER\"]].\\\n",
    "                                    sum(axis = 1)\n",
    "\n",
    "#2005 - first replace 99 with 0 and then add all fields\n",
    "appl_2005 = [\"TVCOLOR\",\"BIGTV\",\"PLASMANUM\",\"VCR\",\"DVD\",\"NCOMBOVCRDVD\",\"PLAYSTA\",\"STEREO\",\"BOOMBOX\",\"COMPCTST\",\"COMPNTST\",\"OTHSTER\"]\n",
    "for a in appl_2005:\n",
    "    dfs[2005][a] = dfs[2005][a].apply(lambda r : r if r != 99 else 0)\n",
    "\n",
    "dfs[2005]['TVAUDIOEQUIP'] = dfs[2005][appl_2005].sum(axis=1)\n",
    "\n",
    "#2009 - \n",
    "appl_2009 = [\"TVCOLOR\",\"COMBODVR1\",\"DVR1\",\"DIGITSTB1\",\"PLAYSTA1\",\"COMBOVCRDVD1\",\"VCR1\",\"DVD1\",\"TVAUDIOSYS1\",\"OTHERSTB1\",\"CABLESAT2\",\"COMBODVR2\",\"DVR2\",\"DIGITSTB2\",\"PLAYSTA2\",\"COMBOVCRDVD2\",\"VCR2\",\"DVD2\",\"TVAUDIOSYS2\",\"OTHERSTB2\",\"CABLESAT3\",\"COMBODVR3\",\"DVR3\",\"DIGITSTB3\",\"PLAYSTA3\",\"COMBOVCRDVD3\",\"VCR3\",\"DVD3\",\"TVAUDIOSYS3\",\"OTHERSTB3\"]\n",
    "\n",
    "for a in appl_2009:\n",
    "    dfs[2009][a] = dfs[2009][a].apply(lambda r : r if r != -2 else 0)\n",
    "\n",
    "dfs[2009]['TVAUDIOEQUIP'] = dfs[2009][appl_2009].sum(axis=1)\n",
    "\n",
    "#2015\n",
    "appl_2015 = [\"TVCOLOR\",\"CABLESAT\",\"COMBODVR\",\"SEPDVR\",\"PLAYSTA\",\"DVD\",\"VCR\",\"INTSTREAM\",\"TVAUDIOSYS\"]\n",
    "for a in appl_2015:\n",
    "    dfs[2015][a] = dfs[2015][a].apply(lambda r : r if r != -2 else 0)\n",
    "\n",
    "dfs[2015]['TVAUDIOEQUIP'] = dfs[2015][appl_2015].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine PC and OFFICE  into one col PCOFFEQUIP\n",
    "\n",
    "pc_2001 = [\"NUMPC\",\"LAPTOPPC\",\"PCPRINT\",\"FXCOPIER\",\"FAX\",\"COPIER\"]\n",
    "for p in pc_2001:\n",
    "    dfs[2001][p] = dfs[2001][p].apply(lambda r : r if r != 99 else 0)\n",
    "    dfs[2001][p] = dfs[2001][p].apply(lambda r : r if r != 9 else 0)\n",
    "    \n",
    "dfs[2001]['PCOFFEQUIP'] = dfs[2001][pc_2001].sum(axis = 1)\n",
    "\n",
    "#2005 - first replace 99 or 9 with 0 and then add all fields\n",
    "pc_2005 = [\"NUMPC\",\"LAPTOPPC\",\"PCPRINT\",\"FXCOPIER\",\"FAX\",\"COPIER\"]\n",
    "monitor = [\"MONITOR1\",\"MONITOR2\",\"MONITOR3\"]\n",
    "for p in pc_2005:\n",
    "    dfs[2005][p] = dfs[2005][p].apply(lambda r : r if (r != 99) else 0)\n",
    "    dfs[2005][p] = dfs[2005][p].apply(lambda r : r if (r != 9) else 0)\n",
    "for m in monitor:\n",
    "    dfs[2005][m] = dfs[2005][m].apply(lambda r : r if (r != 99) else 0)\n",
    "    dfs[2005][m] = dfs[2005][m].apply(lambda r : r if (r != 9) else 0)\n",
    "    dfs[2005][m] = dfs[2005][m].apply(lambda r : r if (r != 2) else 1)    \n",
    "    \n",
    "dfs[2005]['PCOFFEQUIP'] = dfs[2005][pc_2005].sum(axis = 1) + dfs[2005][monitor].sum(axis = 1)\n",
    "\n",
    "#2009 - \n",
    "pc_2009 = [\"NUMPC\",\"PCPRINT\",\"FAX\",\"COPIER\"]\n",
    "monitor = [\"MONITOR1\",\"MONITOR2\",\"MONITOR3\"]\n",
    "for p in pc_2009:\n",
    "    dfs[2009][p] = dfs[2009][p].apply(lambda r : r if (r != -2) else 0)\n",
    "for m in monitor:\n",
    "    dfs[2009][m] = dfs[2009][m].apply(lambda r : r if (r != 0) else 1)\n",
    "    dfs[2009][m] = dfs[2009][m].apply(lambda r : r if (r != -2) else 0)    \n",
    "    \n",
    "dfs[2009]['PCOFFEQUIP'] = dfs[2009][pc_2009].sum(axis = 1) + dfs[2009][monitor].sum(axis = 1)\n",
    "\n",
    "#2015\n",
    "pc_2015 = [\"DESKTOP\",\"NUMLAPTOP\",\"NUMTABLET\",\"ELPERIPH\"]\n",
    "\n",
    "dfs[2015]['PCOFFEQUIP'] = dfs[2015][pc_2015].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NUMSMPHONE     8\n",
       "CELLPHONE     10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[2015][[\"NUMSMPHONE\",\"CELLPHONE\"]].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine All phone and phone related   into one col PHONE\n",
    "\n",
    "ph_01_05 = [\"NOCORD\",\"CELLPHON\",\"ANSMACH\"]\n",
    " \n",
    "#2001\n",
    "dfs[2001]['PHONE'] = dfs[2001][ph_01_05].sum(axis = 1)\n",
    "#2005\n",
    "dfs[2005]['PHONE'] = dfs[2005][ph_01_05].sum(axis = 1)\n",
    "\n",
    "#2009 - \n",
    "ph_2009 = [\"NOCORD\",\"ANSMACH\"]\n",
    "dfs[2009]['PHONE'] = dfs[2009][ph_2009].sum(axis = 1)\n",
    "\n",
    "#2015\n",
    "ph_2015 = [\"NUMSMPHONE\",\"CELLPHONE\"]\n",
    "\n",
    "dfs[2015]['PHONE'] = dfs[2015][ph_2015].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n",
      "1090\n",
      "248\n",
      "277\n"
     ]
    }
   ],
   "source": [
    "print(len(dfs[2001].columns))\n",
    "print(len(dfs[2005].columns))\n",
    "print(len(dfs[2009].columns))\n",
    "print(len(dfs[2015].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnl_cols = pd.read_csv(os.path.join(codebook_path, final_colList), header= None, names = ['cols']).cols.tolist()\n",
    "len(fnl_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns with NAN Values in 2001 datase : []\n",
      "columns with NAN Values in 2005 datase : ['ORIGELS', 'KAVALNG', 'ORIGNGQ', 'CUFEETNG', 'BTUNG', 'BTUNGOTH', 'BTUNGWTH', 'BTUNGSPH', 'DOLLARNG', 'DOLNGOTH', 'DOLNGWTH', 'DOLNGSPH']\n",
      "columns with NAN Values in 2009 datase : []\n",
      "columns with NAN Values in 2015 datase : []\n"
     ]
    }
   ],
   "source": [
    "print(f\"columns with NAN Values in 2001 datase : {dfs[2001].columns[dfs[2001].isna().any()].tolist()}\")\n",
    "print(f\"columns with NAN Values in 2005 datase : {dfs[2005].columns[dfs[2005].isna().any()].tolist()}\")\n",
    "print(f\"columns with NAN Values in 2009 datase : {dfs[2009].columns[dfs[2009].isna().any()].tolist()}\")\n",
    "print(f\"columns with NAN Values in 2015 datase : {dfs[2015].columns[dfs[2015].isna().any()].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns with NAN Values in 2005 dataset after NA FIX : ['ORIGELS', 'KAVALNG', 'ORIGNGQ', 'CUFEETNG', 'BTUNG', 'BTUNGOTH', 'BTUNGWTH', 'BTUNGSPH', 'DOLLARNG', 'DOLNGOTH', 'DOLNGWTH', 'DOLNGSPH']\n"
     ]
    }
   ],
   "source": [
    "# This exists for only for 2005\n",
    "# for the columns with NA, fill it with Mean value of the column\n",
    "dfs[2005].fillna(dfs[2005].mean(), inplace = True)\n",
    "\n",
    "print(f\"columns with NAN Values in 2005 dataset after NA FIX : {dfs[2005].columns[dfs[2005].isna().any()].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_frame = [dfs[2001][fnl_cols],dfs[2005][fnl_cols],dfs[2009][fnl_cols],dfs[2015][fnl_cols]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5686, 199)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[2015][fnl_cols].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Plan shapes are not aligned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-0eb66242da1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# combined_DF = pd.DataFrame(columns = fnl_cols)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# combined_DF = dfs_list[2].append(dfs_list[3], ignore_index = True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfs_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                        copy=copy, sort=sort)\n\u001b[1;32m--> 226\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    421\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m    422\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                 copy=self.copy)\n\u001b[0m\u001b[0;32m    424\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   5404\u001b[0m     \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5406\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin_units\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconcat_plan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5408\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mcombine_concat_plans\u001b[1;34m(plans, concat_axis)\u001b[0m\n\u001b[0;32m   5697\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mnum_ended\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5698\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnum_ended\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5699\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Plan shapes are not aligned\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5701\u001b[0m             \u001b[0mplacements\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnext_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Plan shapes are not aligned"
     ]
    }
   ],
   "source": [
    "# combined_DF = pd.DataFrame(columns = fnl_cols)\n",
    "# combined_DF = dfs_list[2].append(dfs_list[3], ignore_index = True)\n",
    "combined_df = pd.concat(dfs_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns with NAN Values in combined datase : []\n"
     ]
    }
   ],
   "source": [
    "print(f\"columns with NAN Values in combined datase : {combined_df.columns[combined_df.isna().any()].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Count of records in Combined dataset: 26973\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Count of records in Combined dataset: {combined_df.DOEID.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write file to csv\n",
    "combined_df.to_csv(os.path.join(\"./dataforfinalproject\", \"RECS_COMBINED_DATA.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
